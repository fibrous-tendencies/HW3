{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 3 | Generative Adversarial Networks| 15 Points Total | Due October 31 @ 11:59 pm\n",
    "\n",
    "In this homework you will be taking a look the architecture, loss functions, data generation, and training of different GAN models. First you will build a generic GAN to generate new unconditional samples from MNIST. Then you will build [Pix2Pix](https://phillipi.github.io/pix2pix/) model that we looked at in class previously using an existing dataset. Finally, you will need to generate or collect your own dataset and train a pix2pix model that does conditional image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by importing the necessary libraries for this assignment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import torchinfo\n",
    "import urllib.request\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also check if a GPU is available and set the device accordingly for mac/windows users\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 | Unconditional GAN\n",
    "\n",
    "In this section you will define a simple GAN model for unconditional image generation. After training, our expectation is that we should be able to feed in random noise and get back a sample that is convincingly from the distribution of the training dataset.\n",
    "\n",
    "A GAN is made up of two competing models. These models are generally referred to as the **Generator** and the **Discriminator**. The specific model setup is not defined in [Goodfellow et al.](https://arxiv.org/abs/1406.2661), but we can infer many of the details from the paper and build two MLP style models with a reasonably large capacity. *Reasonably large* is a pretty arbitrary specification. Feel free to experiment with the input/output sizes of the layers in the generator and discriminator to see how it affects the results of your model. \n",
    "\n",
    " Let's start by defining the `Generator` with it's `init` and `forward` methods. This model should just take in a noise vector of `z_dim` and upsample this until it outputs an image the same shape as the MNIST images (28x28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        #Store the dimension of the noise vector\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        #Define the layers of the generator\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 28 * 28),  # MNIST images are 28x28\n",
    "            nn.Tanh()  # Output values between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1, 1, 28, 28) # Reshape output to image shape (batch_size, channels, x_dim, y_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the `Discriminator` which should take in either a real image from the training set or a synthetic image from the `Generator` and make a binary prediction of whether it is a real or fake image. \n",
    "\n",
    "Goodfellow et. al. suggests the use of dropout layers to promote training stability. They also recommend the use of [maxout activations](https://paperswithcode.com/method/maxout), however we will use [Leaky ReLU](https://paperswithcode.com/method/leaky-relu) instead. \n",
    "\n",
    "[Leaky ReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) looks like the standard ReLU, but has a slight slope for values below 0 to prevent [*vanishing gradients*](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.LeakyReLU(0.2), # Leaky ReLU activation function with a slope of 0.2\n",
    "            nn.Dropout(0.3),  # Dropout layer\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2), # Leaky ReLU activation function with a slope of 0.2\n",
    "            nn.Dropout(0.3),  # Dropout layer\n",
    "            nn.Linear(256, 1),  # Output a single value\n",
    "            nn.Sigmoid()  # Output prediction values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have our two models defined. Next we will define the loss function and optimizer for our GAN. Recall that the process for training the GAN model is as follows:\n",
    "\n",
    "1. Initialize the generator and discriminator models\n",
    "2. Set the number of training epochs\n",
    "3. Define the loss function (e.g., Binary Cross-Entropy)\n",
    "4. Choose optimizers for both the generator and discriminator (e.g., Adam)\n",
    "\n",
    "5. For each epoch:\n",
    "\n",
    "    For each batch of real images from the training dataset:\n",
    "    \n",
    "        i. Generate random noise (z) as input for the generator\n",
    "        \n",
    "        ii. Use the generator to create fake images (G(z))\n",
    "        \n",
    "        iii. Compute the discriminator's predictions:\n",
    "            - Real images: D(real_images)\n",
    "            - Fake images: D(G(z))\n",
    "        \n",
    "        iv. Calculate the loss for the discriminator:\n",
    "            - Loss_D = - (mean(log(D(real_images))) + mean(log(1 - D(G(z)))))\n",
    "        \n",
    "        v. Update the discriminator's weights using the calculated loss\n",
    "        \n",
    "        vi. Generate new random noise (z) for the generator\n",
    "        vii. Compute the discriminator's predictions for the fake images again: D(G(z))\n",
    "        \n",
    "        viii. Calculate the loss for the generator:\n",
    "            - Loss_G = - mean(log(D(G(z))))\n",
    "        \n",
    "        ix. Update the generator's weights using the calculated loss\n",
    "\n",
    "6. End training after the specified number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you should initialize your generator and discriminator and set up your optimizers for the generator and discriminator. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "\n",
    "generator = ###Your code here\n",
    "discriminator = ###Your code here\n",
    "\n",
    "#We will set the these variables here for use later\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "\n",
    "#The loss function for your model can be defined as a Binary Cross Entropy loss\n",
    "#For details on why we can use this loss everywhere see: https://towardsdatascience.com/decoding-the-basic-math-in-gan-simplified-version-6fb6b079793\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go much further it it would be nice if we have a function that we can call to plot some random samples with our generator. This will let us check the progress of the optimization as it progresses. \n",
    "\n",
    "This function will be called `generate_images(generator, num_images, noise_dim, device)` and it should not have a return value, it should just plot a set of images. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(generator, num_images, noise_dim, device):\n",
    "    #First you need to create a set of random z vectors that have the shape (num_images, noise_dim)\n",
    "    #Hint: use torch.randn()\n",
    "    ###Your code here\n",
    "\n",
    "    #Next we will stop tracking gradients for a bit to generate the images\n",
    "    #Hint: look up .cpu()\n",
    "    with torch.no_grad():\n",
    "        generated_images = ###Your code here\n",
    "\n",
    "    #Now that you have your generated images you can plot them in a subplot\n",
    "    #Hint: use plt.subplots()\n",
    "    \"\"\"\n",
    "    Your Code here\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to define our training loop. Let's also set this up as a function we can call later on. You will need to fill out a number of sections in this training loop function. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, n_epochs, batch_size, lr, loss_fn, optimizer_G, optimizer_D, dataloader, device):\n",
    "    z_dim = generator.z_dim\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            # Move real images to device\n",
    "            real_images = ###Your code here\n",
    "\n",
    "            #Create labels for real and fake images\n",
    "            real_labels = torch.ones(real_images.shape[0]).unsqueeze(-1).to(device)\n",
    "            fake_labels = torch.zeros(real_images.shape[0]).unsqueeze(-1).to(device)\n",
    "\n",
    "            #Zero out the gradients for the discriminator optimizer\n",
    "            ###Your code here\n",
    "\n",
    "            #Have the discriminator make predictions on the real images\n",
    "            real_preds = ###Your code here\n",
    "\n",
    "            #Calculate the discriminator loss using the real predictions and the real labels\n",
    "            loss_D_real = ###Your code here, hint: you already defined the loss function and passed it as an argument to train_gan\n",
    "\n",
    "            #Generate random noise for the generator\n",
    "            z = ###Your code here, Hint: use torch.randn() and think about what shape this tensor should have to be fed into the generator.\n",
    "\n",
    "            #Generate a fake image using your generator.\n",
    "            fake_images = ###Your code here\n",
    "\n",
    "            #Have the discriminator make predictions on the fake images\n",
    "            fake_preds = ###Your code here, Hint: you should detach the fake images using .detach()\n",
    "\n",
    "            #Calculate the discriminator loss using the fake predictions and the fake labels\n",
    "            loss_D_fake = ###Your code here\n",
    "\n",
    "            #Calculate the total discriminator loss\n",
    "            loss_D = loss_D_real + loss_D_fake\n",
    "\n",
    "            #Backpropagate the discriminator loss\n",
    "            ###Your code here\n",
    "\n",
    "            #Update the discriminator weights\n",
    "            ###Your code here, Hint: use .step()\n",
    "\n",
    "            #Zero out the gradients for the generator optimizer\n",
    "            ###Your code here\n",
    "\n",
    "            #Have the discriminator make predictions on the fake images again\n",
    "            #This time you need gradients so do not .detach()\n",
    "            fake_preds = ###Your code here\n",
    "\n",
    "            #Calculate the generator loss on the fake predictions and the real labels using the loss function\n",
    "            loss_G = ###Your code here\n",
    "\n",
    "            #Backpropagate the generator loss\n",
    "            ###Your code here\n",
    "\n",
    "            #Update the generator weights\n",
    "            ###Your code here\n",
    "\n",
    "            #Print the losses for each batch\n",
    "            if i % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Epoch [{epoch+1}/{n_epochs}] | Batch [{i+1}/{len(dataloader)}] | Loss_D: {loss_D:.4f} | Loss_G: {loss_G:.4f}\")\n",
    "                generate_images(generator, 10, z_dim, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model defined, we need some data to train our model. Let's use the MNIST dataset and create a dataloader that transforms the images to a tensor and normalizes them to -1 to 1 to match the expected range of outputs from the generator. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a transform using transforms.Compose to cast the data to a tensor and to normalize the data \n",
    "#Hint: the values in normalize should all be 0.5\n",
    "transform = ###Your code here\n",
    "\n",
    "#We will use the datasets.MNIST database, no need to write any code here\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "#Finally you can create your dataloader using your dataset from above\n",
    "#Make sure shuffle is set to true\n",
    "dataloader = ###Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should just be able to call `train_gan()` with all of the appropriate inputs. You should have defined all the arguments already. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Your code here, call train_gan with the appropriate arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you training was successful then you should be able to run your `generate_images` function to plot a range of samples from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(generator, 10, z_dim, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens if we try to interpolate between random latent vectors! (1 point)\n",
    "\n",
    "For this section you will need to first generate two random vectors `z1` and `z2`\n",
    ">Hint: use torch.randn()\n",
    "\n",
    "Then you need to create a set of interpolated latent vectors between `z1` and `z2`. You should create 10 steps between the latent codes to sample.\n",
    ">Hint: use [torch.linspace()](https://pytorch.org/docs/stable/generated/torch.linspace.html) and [torch.lerp()](https://pytorch.org/docs/stable/generated/torch.lerp.html) to generate these latent vectors.\n",
    "\n",
    "Then you need to use your trained generator to create new images given each of these latent vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First you need to generate two random vectors with the shape (1, z_dim)\n",
    "z1 = torch.randn(1, z_dim).to(device)\n",
    "z2 = torch.randn(1, z_dim).to(device)\n",
    "\n",
    "\n",
    "steps = 10\n",
    "interpolation_range = torch.linspace(0, 1, steps=steps).unsqueeze(1).to(device)\n",
    "print(interpolation_range.shape)\n",
    "interpolated_z = torch.lerp(z1, z2, interpolation_range).squeeze(0).to(device)\n",
    "\n",
    "#Now let's generate images for the interpolated latent vectors\n",
    "interpolated_images = generator(interpolated_z)\n",
    "print(interpolated_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's plot the interpolated images\n",
    "fig, axes = plt.subplots(1, steps, figsize=(10, 10))\n",
    "for i in range(steps):\n",
    "    axes[i].imshow(interpolated_images[i].squeeze().detach().cpu(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 | Conditional GAN | pix2pix\n",
    "\n",
    "In this section you will be building a **cGAN** based on the paper [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) by Isola et. al. The code here is adapted from [Philip Isola's github repo](https://github.com/phillipi/pix2pix). \n",
    "\n",
    "The general setup for this model will look very similar to the unconditional GAN model above. You will again have a `Generator` and a `Discriminator` model, however this time you will need to feed the input image into the discriminator model as your condition.\n",
    "\n",
    "![pix2pix](imgs/pix2pix.png)\n",
    "\n",
    "The `Generator` used is a `U-Net` architecture. A `U-Net` model architeecture is described in [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) and follows a symmetrical convolutional encoder-decoder architecture. It also includes skip connections to allow information to 'shortcut' across the network. \n",
    "\n",
    "\n",
    "\n",
    "The `Discriminator` uses a convolutional `PatchGAN` classifier. The `PatchGAN` is claimed by the authors to work on a wider range of problems since it only penalizes structure at the scale of image patches. Here an image patch is just meant to be some smaller subsection of an image. The best results from the paper were achieved using a PatchGAN with 'receptive-fields' that were 70 pixels x 70 pixels. \n",
    "\n",
    "![patch_gan.png](imgs/patch_gan.png)\n",
    "\n",
    "In the paper they describe the network architecture using a compact notation:\n",
    "\n",
    ">Let Ck denote a Convolution-BatchNorm-ReLU layer  with k filters. CDk denotes a Convolution-BatchNormDropout-ReLU layer with a dropout rate of 50%. All convolutions are 4 × 4 spatial filters applied with stride 2. Convolutions in the encoder, and in the discriminator, downsample  by a factor of 2, whereas in the decoder they upsample by a  factor of 2.\n",
    "\n",
    ">The encoder-decoder architecture consists of:  encoder:  `C64-C128-C256-C512-C512-C512-C512-C512`  decoder:  `CD512-CD512-CD512-C512-C256-C128-C64`  After the last layer in the decoder, a convolution is applied to map to the number of output channels (3 in general,  except in colorization, where it is 2), followed by a Tanh  function. As an exception to the above notation, BatchNorm is not applied to the first C64 layer in the encoder.  All ReLUs in the encoder are leaky, with slope 0.2, while  ReLUs in the decoder are not leaky.  The U-Net architecture is identical except with skip connections between each layer i in the encoder and layer n − i  in the decoder, where n is the total number of layers. The  skip connections concatenate activations from layer i to  layer n − i. This changes the number of channels in the  decoder:  U-Net decoder:  `CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128`\n",
    "\n",
    ">The 70 × 70 discriminator architecture is:  `C64-C128-C256-C512`  After the last layer, a convolution is applied to map to  a 1-dimensional output, followed by a Sigmoid function.  As an exception to the above notation, BatchNorm is not  applied to the first C64 layer. All ReLUs are leaky, with  slope 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these modules I have written out all of the steps in sequence to make it easier to follow the flow of data through the model. In practice you should write some functions that generate these models automatically using some other kind of logic (i.e. for loops and nn.Sequential()). The code from github does this model generation in a way that handles a variety of scenarios, however it is not the most straightforward code to read. The architecture defined below is designed for input images that are 256x256. If you try to run smaller images through the network you will get an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "        self.e1 = nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.e2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.be2 = nn.InstanceNorm2d(128)\n",
    "\n",
    "        self.e3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.be3 = nn.InstanceNorm2d(256)\n",
    "\n",
    "        self.e4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.be4 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.e5 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.be5 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.e6 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.be6 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.e7 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.be7 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.e8 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.be8 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.d1 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bd1 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.d2 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bd2 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.d3 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bd3 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.d4 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bd4 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        self.d5 = nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bd5 = nn.InstanceNorm2d(256)\n",
    "\n",
    "        self.d6 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bd6 = nn.InstanceNorm2d(128)\n",
    "\n",
    "        self.d7 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bd7 = nn.InstanceNorm2d(64)\n",
    "\n",
    "        self.d8 = nn.ConvTranspose2d(128, output_nc, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.leaky_relu(self.e1(x), 0.2)  # Apply first convolution and Leaky ReLU activation\n",
    "        x2 = F.leaky_relu(self.be2(self.e2(x1)), 0.2)  # Apply second convolution, instance normalization, and Leaky ReLU\n",
    "        x3 = F.leaky_relu(self.be3(self.e3(x2)), 0.2)  # Apply third convolution, instance normalization, and Leaky ReLU\n",
    "        x4 = F.leaky_relu(self.be4(self.e4(x3)), 0.2)  # Apply fourth convolution, instance normalization, and Leaky ReLU\n",
    "        x5 = F.leaky_relu(self.be5(self.e5(x4)), 0.2)  # Apply fifth convolution, instance normalization, and Leaky ReLU\n",
    "        x6 = F.leaky_relu(self.be6(self.e6(x5)), 0.2)  # Apply sixth convolution, instance normalization, and Leaky ReLU\n",
    "        x7 = F.leaky_relu(self.be7(self.e7(x6)), 0.2)  # Apply seventh convolution, instance normalization, and Leaky ReLU\n",
    "        x8 = F.leaky_relu(self.e8(x7), 0.2)  # Apply eighth convolution and Leaky ReLU\n",
    "\n",
    "        x = F.relu(self.bd1(self.d1(x8)))  # Upsample with first transposed convolution and apply ReLU\n",
    "        x = F.dropout2d(x, 0.5)  # Apply dropout for regularization\n",
    "        x = F.relu(self.bd2(self.d2(torch.cat([x, x7], 1))))  # Concatenate with skip connection and apply second upsample\n",
    "        x = F.dropout2d(x, 0.5)  # Apply dropout for regularization\n",
    "        x = F.relu(self.bd3(self.d3(torch.cat([x, x6], 1))))  # Concatenate with skip connection and apply third upsample\n",
    "        x = F.relu(self.bd4(self.d4(torch.cat([x, x5], 1))))  # Concatenate with skip connection and apply fourth upsample\n",
    "        x = F.relu(self.bd5(self.d5(torch.cat([x, x4], 1))))  # Concatenate with skip connection and apply fifth upsample\n",
    "        x = F.relu(self.bd6(self.d6(torch.cat([x, x3], 1))))  # Concatenate with skip connection and apply sixth upsample\n",
    "        x = F.relu(self.bd7(self.d7(torch.cat([x, x2], 1))))  # Concatenate with skip connection and apply seventh upsample\n",
    "        x = F.tanh(self.d8(torch.cat([x, x1], 1)))  # Concatenate with skip connection and apply final transposed convolution with Tanh activation\n",
    "        return x\n",
    "    \n",
    "class DiscriminatorPatchGAN(nn.Module):\n",
    "    def __init__(self, input_nc, condition_nc):\n",
    "        super(DiscriminatorPatchGAN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_nc + condition_nc, 64, kernel_size=4, stride=2, padding=1),  # Combine input and condition\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)  # Output a single value\n",
    "        )\n",
    "\n",
    "    def forward(self, img, condition):\n",
    "        # Concatenate the input image and condition along the channel dimension\n",
    "        x = torch.cat([img, condition], 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will initialize our generator and discriminator models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_unet = GeneratorUNet(input_nc=3, output_nc=3).to(device)\n",
    "discriminator_patchgan = DiscriminatorPatchGAN(input_nc=3, condition_nc=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(generator_unet, (1, 3, 256, 256), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(discriminator_patchgan,((1, 3, 256, 256), (1,3, 256, 256)), device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize the weights of the networks to match the paper. We will just use the init_weights function they defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "beta1 = 0.5 #momentum\n",
    "beta2 = 0.999 #alpha\n",
    "\n",
    "init_weights(generator_unet, init_type='normal', init_gain=0.02)\n",
    "init_weights(discriminator_patchgan, init_type='normal', init_gain=0.02)\n",
    "\n",
    "g_optimizer = optim.Adam(generator_unet.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "d_optimizer = optim.Adam(discriminator_patchgan.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download some data and create the dataloader. You will be using the facades dataset for this homework since it is nice and small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_facades(url, save_path):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading dataset from {url}...\")\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"Dataset already downloaded.\")\n",
    "\n",
    "# URL for the edges2shoes dataset\n",
    "facades_url = 'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz'\n",
    "save_path = './datasets/facades.tar.gz'\n",
    "\n",
    "download_facades(facades_url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset\n",
    "import tarfile\n",
    "\n",
    "def extract_dataset(tar_path, extract_path):\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "\n",
    "if not os.path.exists('./datasets/facades'):\n",
    "    extract_dataset(save_path, './datasets')\n",
    "else:\n",
    "    print(\"Dataset already extracted.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, instead of using an existing dataset we will build our own. The first step to creating a dataset is defining a class that inherits from `Dataset` which is part of torch.utils.data. This class has a few critical sections. \n",
    "\n",
    "The first is __init__ which is called when a new FacadesDataset is generated. \n",
    "\n",
    "Inside of this particular dataset class there are a few internal variables declared. Root is the directory that contains the images for the dataset. Transform stores the transformations that should be applied each time __getitem__ is called. Images is a list that stores all of the paths to the image files of interest. \n",
    "\n",
    "Inside of __init__ is also a for loop that is run once when the FacadesDataset is created.\n",
    "\n",
    "The next important section is __len__. This just allows you to query the dataset to figure out its length using len(dataset).\n",
    "\n",
    "Finally the __getitem__ function is defined. This allows you to get a particular item from the dataset by index. In this FacadesDataset calling __getitem__ opens the image at the file location stored in self.images = [][idx] and applies any given transformations to the image. This is very useful if your images are large or you have a big batch size as it prevents you from having to load all of your images into memory on your computer.\n",
    "\n",
    "The class is just a placeholder, it does not do anything until it is instantiated which we will do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "\n",
    "        for root, dirs, files in os.walk(root):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    self.images.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your Dataset class defined you can instantiate it. To create your dataset you need to pass in the folder location of your images and the transforms you wish to apply to the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will define the transforms to convert our image to a tensor and to normalize our image data.\n",
    "transform = transforms.Compose([    \n",
    "    transforms.ToTensor(), # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "#Now we can actually create our dataset.\n",
    "facades_dataset = FacadesDataset(root='./datasets/facades/', transform=transform)\n",
    "\n",
    "#We can get the length of the dataset using len()\n",
    "print(len(facades_dataset))\n",
    "\n",
    "#We can get a particular item from the dataset using an idx\n",
    "print(facades_dataset[0])\n",
    "\n",
    "#The dataloader is another special function that pytorch uses for creating mini-batches of data for training.\n",
    "#Batch size sets the number of samples that should be returned each time the dataloader is iterated. \n",
    "#Shuffle = True causes the idx or list of idx to be randomized for each batch, otherwise the samples are called in order\n",
    "facades_dataloader = DataLoader(facades_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "#After you have created your DataLoader \n",
    "#You can manually iterate through the newly defined dataloader using the syntax below. \n",
    "sample = next(iter(facades_dataloader))\n",
    "print(sample.shape)\n",
    "\n",
    "#The dataloader is what we will pass into our training function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample from the facades dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we need to permute the axes of the image to order them correctly for plt.imshow()\n",
    "#We also un-normalize the image so the color is not shifted when we draw the image\n",
    "plt.imshow((sample[0].permute(1, 2, 0) + 1.0) / 2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the dataset is set up with pairs of images composed together into one frame. This means that we just need to choose half of our image to be fed into the generator and the other half will serve as the ground truth. \n",
    "\n",
    "Before we define the training loop we can also set up a few extra utility functions for plotting and doing some formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just a wrapper function for the loss function\n",
    "def GAN_loss(output, target):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(output, target)\n",
    "    return loss\n",
    "\n",
    "#This is another wrapper function\n",
    "def l1_loss(output, target):\n",
    "    criterion = nn.L1Loss()\n",
    "    loss = criterion(output, target)\n",
    "    return loss\n",
    "\n",
    "#This function will be used to plot a set of images.\n",
    "def plot_sample_grid(condition, fake_photo, real_photo):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].imshow(condition)\n",
    "    axs[0].set_title('Condition')\n",
    "    axs[1].imshow(fake_photo)\n",
    "    axs[1].set_title('Fake Photo')\n",
    "    axs[2].imshow(real_photo)\n",
    "    axs[2].set_title('Real Photo')\n",
    "    plt.show()\n",
    "\n",
    "#This function will help when we want to turn on or off the gradient tracking for our parameters\n",
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "#This function simplifies converting from a tensor to an image and does the un-normalization so we can see the correct colors\n",
    "#From https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/util.py\n",
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (tensor) --  the input image tensor array\n",
    "        imtype (type)        --  the desired type of the converted numpy array\n",
    "    \"\"\"\n",
    "    if not isinstance(input_image, np.ndarray):\n",
    "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
    "            image_tensor = input_image.data\n",
    "        else:\n",
    "            return input_image\n",
    "        image_numpy = image_tensor.cpu().float().numpy()  # convert it into a numpy array\n",
    "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
    "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
    "    else:  # if it is a numpy array, do nothing\n",
    "        image_numpy = input_image\n",
    "    return image_numpy.astype(imtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to defining the training loop.\n",
    "\n",
    "In this model you will be trying to go from facade labels to photos of facades. The goal of the generator is to create realistic looking images of facades, given a labeled facade image. The goal of the discriminator is to determine if the generated image is a real photo of a facade or a generated photo of a facade. If you would like to swap the order to generate labels from images you can do so by modifying how the data is sliced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pix2pix(generator, discriminator, dataloader, device, lambda_l1=100, num_epochs=25):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for i, data in enumerate(dataloader):\n",
    "            #Take half of the image as the input and the other half as the ground truth\n",
    "            real_photo = data[..., :256].to(device)\n",
    "            condition = data[..., 256:].to(device) \n",
    "\n",
    "            # Start Generation Here\n",
    "            # Train Discriminator\n",
    "            set_requires_grad(discriminator, True)\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            # Fake pairs\n",
    "            fake_photo = generator(condition)\n",
    "            pred_fake = discriminator(fake_photo.detach(), condition)\n",
    "            fake = torch.zeros_like(pred_fake, device=device)\n",
    "            loss_D_fake = GAN_loss(pred_fake, fake)\n",
    "\n",
    "            # Real pairs\n",
    "            pred_real = discriminator(real_photo, condition) #The discriminator takes in the real/fake photo and the condition\n",
    "            valid = torch.ones_like(pred_real, device=device)\n",
    "            loss_D_real = GAN_loss(pred_real, valid)    \n",
    "\n",
    "            # Total Discriminator loss\n",
    "            loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "            loss_D.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            set_requires_grad(discriminator, False)\n",
    "            g_optimizer.zero_grad()\n",
    "            pred_fake = discriminator(fake_photo, condition)\n",
    "            loss_G_GAN = GAN_loss(pred_fake, valid)\n",
    "            loss_G_L1 = l1_loss(fake_photo, real_photo) * lambda_l1\n",
    "            loss_G = loss_G_GAN + loss_G_L1\n",
    "            loss_G.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            \n",
    "            # Start Generation Here        \n",
    "            if i % 50 ==0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss_D: {loss_D.item()}, Loss_G_GAN: {loss_G_GAN.item()}, Loss_G_L1: {loss_G_L1.item()}\")\n",
    "                with torch.no_grad():            \n",
    "                    fake_image = generator(condition)[0]\n",
    "                    fake_photo_img = tensor2im(fake_image)\n",
    "                    real_photo_img = tensor2im(real_photo[0])\n",
    "                    plot_sample_grid(tensor2im(condition.squeeze(0)), fake_photo_img, real_photo_img)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can call our train_pix2pix function and proceed with training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pix2pix(generator_unet, discriminator_patchgan, facades_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! With any luck (and a bit of training time) you should begin to see improvements in the generation of fake photos given a labeled condition image as input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 | pix2pix with your own dataset | 5 points\n",
    "\n",
    "In this section of the assignment you will need to create your own dataset and use it for training a pix2pix model. Your dataset needs to have a minimum of 30 pairs of images. With only 30 image pairs you likely won't get amazing results from your model, but this will help serve as a template if you would like to build a larger dataset at a later time. \n",
    "\n",
    "I will leave it up to you to decide how you would like to generate your image pairs, but the size of your images should be 256x256 pixels and the image pairs should be split into two folders where the order of images in one folder matches the order in the other folder. I.e. you should have a folder of images_A and a folder of images_B and the filenames in each folder should be set up such that they are in the same order in both folders. Using a numerical file naming scheme is your best approach here. If you choose to have one or both of your datasets be in grayscale then you will need to modify the input/output number of channels. \n",
    "\n",
    "You should add your image folders to the same folder that contains the previously downloaded datasets. \n",
    "\n",
    "The rest of the setup of the pix2pix model has already been done for you above so the main thing you need to do is to define the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I have already defined the DIY dataset, note the differences with the facade dataset class\n",
    "class DIY_Dataset(Dataset):\n",
    "    def __init__(self, root_A, root_B, transform=None):\n",
    "        self.root_a = root_A\n",
    "        self.root_b = root_B\n",
    "        self.transform = transform\n",
    "        self.images_A = []\n",
    "        self.images_B = []\n",
    "\n",
    "        for root_A, dirs, files in os.walk(root_A):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    self.images_A.append(os.path.join(root_A, file))\n",
    "        \n",
    "        for root_B, dirs, files in os.walk(root_B):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    self.images_B.append(os.path.join(root_B, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.images_A),len(self.images_B))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_a = Image.open(self.images_A[idx])\n",
    "        image_b = Image.open(self.images_B[idx])\n",
    "        if self.transform:\n",
    "            image_a = self.transform(image_a)\n",
    "            image_b = self.transform(image_b)\n",
    "        return image_a, image_b     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will define the transforms to convert our image to a tensor and to normalize our image data.\n",
    "transform_DIY = transforms.Compose([    \n",
    "    transforms.ToTensor(), # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "#Now we can actually create our dataset.\n",
    "DIY_dataset = ###Your code here\n",
    "\n",
    "#We can get the length of the dataset using len()\n",
    "print(len(DIY_dataset))\n",
    "\n",
    "#We can get a particular item from the dataset using an idx\n",
    "print(DIY_dataset[0][0].shape,DIY_dataset[1][0].shape)\n",
    "\n",
    "#Define you dataloader\n",
    "DIY_dataloader = DataLoader(DIY_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pix2pix_DIY(generator, discriminator, dataloader, d_opt, g_opt, device, lambda_l1=100, num_epochs=25):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for i, data in enumerate(dataloader):\n",
    "            #Your dataloader should be set up to return two images instead of one combined image that needs to be split in two.\n",
    "            #If you would like to switch the order of your model then you can just swap these indices\n",
    "            real_photo = data[0].to(device)\n",
    "            condition = data[1].to(device) \n",
    "\n",
    "            # Start Generation Here\n",
    "            # Train Discriminator\n",
    "            set_requires_grad(discriminator, True)\n",
    "            d_opt.zero_grad()\n",
    "\n",
    "            # Fake pairs\n",
    "            fake_photo = generator(condition)\n",
    "            pred_fake = discriminator(fake_photo.detach(), condition)\n",
    "            fake = torch.zeros_like(pred_fake, device=device)\n",
    "            loss_D_fake = GAN_loss(pred_fake, fake)\n",
    "\n",
    "            # Real pairs\n",
    "            pred_real = discriminator(real_photo, condition) #The discriminator takes in the real/fake photo and the condition\n",
    "            valid = torch.ones_like(pred_real, device=device)\n",
    "            loss_D_real = GAN_loss(pred_real, valid)    \n",
    "\n",
    "            # Total Discriminator loss\n",
    "            loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "            loss_D.backward()\n",
    "            d_opt.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            set_requires_grad(discriminator, False)\n",
    "            g_opt.zero_grad()\n",
    "            pred_fake = discriminator(fake_photo, condition)\n",
    "            loss_G_GAN = GAN_loss(pred_fake, valid)\n",
    "            loss_G_L1 = l1_loss(fake_photo, real_photo) * lambda_l1\n",
    "            loss_G = loss_G_GAN + loss_G_L1\n",
    "            loss_G.backward()\n",
    "            g_opt.step()\n",
    "            \n",
    "            # Start Generation Here        \n",
    "            if i % 50 ==0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss_D: {loss_D.item()}, Loss_G_GAN: {loss_G_GAN.item()}, Loss_G_L1: {loss_G_L1.item()}\")\n",
    "                with torch.no_grad():            \n",
    "                    fake_image = generator(condition)[0]\n",
    "                    fake_photo_img = tensor2im(fake_image)\n",
    "                    real_photo_img = tensor2im(real_photo[0])\n",
    "                    plot_sample_grid(tensor2im(condition.squeeze(0)), fake_photo_img, real_photo_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_unet_DIY = GeneratorUNet(input_nc=3, output_nc=3).to(device)\n",
    "discriminator_patchgan_DIY = DiscriminatorPatchGAN(input_nc=3, condition_nc=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "beta1 = 0.5 #momentum\n",
    "beta2 = 0.999 #alpha\n",
    "\n",
    "init_weights(generator_unet_DIY, init_type='normal', init_gain=0.02)\n",
    "init_weights(discriminator_patchgan_DIY, init_type='normal', init_gain=0.02)\n",
    "\n",
    "g_optimizer_DIY = optim.Adam(generator_unet_DIY.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "d_optimizer_DIY = optim.Adam(discriminator_patchgan_DIY.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pix2pix_DIY(generator_unet_DIY, discriminator_patchgan_DIY, g_optimizer_DIY, d_optimizer_DIY, DIY_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing HW3! To submit your assignment please save all your files and then commit and sync them to your github repo. Also create a zip file containing everything in the HW3 folder (make sure to include your image files!) and upload this zip file to canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIGN-6197-4197-HW3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
